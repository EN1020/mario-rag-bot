import chromadb
from chromadb.utils import embedding_functions
from google import genai
import os
import streamlit as st  # 1. å¼•å…¥ streamlit

# è¨­å®šé é¢æ¨™é¡Œ
st.set_page_config(page_title="ğŸ„ ç‘ªåˆ©æ­ AI å°ˆå®¶", page_icon="ğŸ„")
st.title("ğŸ„ ç‘ªåˆ©æ­éŠæˆ²çŸ¥è­˜å•ç­” RAG")

# æª¢æŸ¥ API Key
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸ï¼")
    st.stop()

# ==========================================
# 2. åˆå§‹åŒ– (åŠ ä¸Šå¿«å–ï¼Œé¿å…æ¯æ¬¡å°è©±éƒ½é‡è·‘)
# ==========================================
@st.cache_resource
def load_resources():
    print("æ­£åœ¨è¼‰å…¥æ¨¡å‹èˆ‡è³‡æ–™åº«...")
    # åˆå§‹åŒ– Google Client
    client = genai.Client(api_key=api_key)
    
    # åˆå§‹åŒ– Embedding
    emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="paraphrase-multilingual-MiniLM-L12-v2"
    )
    
    # é€£ç·šè³‡æ–™åº« (æ³¨æ„ï¼šDocker å…§çš„è·¯å¾‘)
    chroma_client = chromadb.PersistentClient(path="./mario_db_local")
    collection = chroma_client.get_collection(
        name="mario_knowledge",
        embedding_function=emb_fn
    )
    return client, collection

# è¼‰å…¥è³‡æº
client, collection = load_resources()

# ==========================================
# 3. å®šç¾©å›ç­”é‚è¼¯ (è·ŸåŸæœ¬ä¸€æ¨£)
# ==========================================
def get_answer(question):
    # Step 1: æª¢ç´¢
    results = collection.query(query_texts=[question], n_results=2)
    
    # é€™è£¡åšå€‹å°æ”¹å‹•ï¼šå¦‚æœè·é›¢å¤ªé ï¼Œç›´æ¥å›å‚³æ‰¾ä¸åˆ° (é€™æ˜¯ä¸€å€‹ä¿éšª)
    # ä¸éç‚ºäº†å±•ç¤º RAG æ•ˆæœï¼Œæˆ‘å€‘å…ˆç¶­æŒåŸæœ¬çš„ Prompt æ©Ÿåˆ¶
    retrieved_text = "\n".join(results['documents'][0])
    
    # Step 2: ç”Ÿæˆ
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç‘ªåˆ©æ­éŠæˆ²å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹çš„ã€åƒè€ƒè³‡æ–™ã€‘å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æœè³‡æ–™è£¡æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè«‹èªªã€Œä¸å¥½æ„æ€ï¼Œæˆ‘çš„çŸ¥è­˜åº«è£¡æ²’æœ‰ç›¸é—œè³‡è¨Šã€ã€‚
    
    ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
    {retrieved_text}
    
    ã€ä½¿ç”¨è€…å•é¡Œã€‘ï¼š
    {question}
    """
    try:
        # è«‹è¨˜å¾—ç¢ºèªé€™è£¡çš„æ¨¡å‹åç¨±æ˜¯ä½ å¸³è™Ÿèƒ½ç”¨çš„ (ä¾‹å¦‚ gemini-2.0-flash-exp)
        response = client.models.generate_content(
            model="gemini-3-flash-preview", 
            contents=prompt
        )
        return response.text, retrieved_text
    except Exception as e:
        return f"âŒ éŒ¯èª¤: {e}", ""

# ==========================================
# 4. ç¶²é äº’å‹•å€ (å–ä»£åŸæœ¬çš„ while è¿´åœˆ)
# ==========================================

# å»ºç«‹ä¸€å€‹èŠå¤©è¼¸å…¥æ¡†
user_input = st.chat_input("è«‹è¼¸å…¥é—œæ–¼ç‘ªåˆ©æ­çš„å•é¡Œ...")

if user_input:
    # 1. é¡¯ç¤ºä½¿ç”¨è€…çš„è¨Šæ¯
    with st.chat_message("user"):
        st.write(user_input)

    # 2. é¡¯ç¤º AI æ€è€ƒéç¨‹èˆ‡å›ç­”
    with st.chat_message("assistant"):
        with st.spinner("ğŸ„ ç‘ªåˆ©æ­æ­£åœ¨ç¿»é–±ç™¾ç§‘å…¨æ›¸..."):
            answer, ref_data = get_answer(user_input)
            st.write(answer)
            
            # (é¸ç”¨) é¡¯ç¤ºå®ƒåƒè€ƒäº†ä»€éº¼è³‡æ–™ï¼Œè®“é€™çœ‹èµ·ä¾†æ›´åƒé™¤éŒ¯å·¥å…·
            with st.expander("æŸ¥çœ‹ AI åƒè€ƒäº†å“ªäº›è³‡æ–™"):
                st.info(ref_data)
